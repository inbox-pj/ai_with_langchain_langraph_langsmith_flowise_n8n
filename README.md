# Flowise - Visual LLM Flow Builder - NoCode-LowCode AI Flows

## üöÄ LLM Docker Compose Stack
This project provides a Docker Compose setup for running multiple AI and data services, including Ollama, pgvector, PostgreSQL, Flowise, and n8n.


### Services
- Ollama: Local LLM inference server.
- pgvector: PostgreSQL extension for vector similarity search.
- Postgres: Main PostgreSQL database for n8n and Flowise.
- Flowise: Visual LLM workflow builder.
- n8n: Workflow automation tool.

```bash
    # Start Flowise, pgvector and ollama with Docker
    docker-compose up -d
```
#### Access services:
- Ollama: http://localhost:11434
- Flowise: http://localhost:3000
- n8n: http://localhost:5678
- PostgreSQL:
  - n8n: port 25432, DB n8n, user n8n, password postgres 
  - Flowise: port 25432, DB flowise, user flowise, password postgres 
  - pgvector: port 15432, DB vectord_db, user postgres, password postgres
 
#### Mail Server:
- Server name : test.mailu.io
- IP address : 173.249.45.89
- Webmail : https://test.mailu.io/webmail/
- Admin UI : https://test.mailu.io/admin/
- Admin login : admin@test.mailu.io
- Admin password : MailuDemo

# LangChain ü¶úüîó
<img width="643" height="432" alt="image" src="https://github.com/user-attachments/assets/e711c3af-528f-44a6-8985-3383fdd7d0d5" />

# LangSmith  ü¶úüîó
<img width="1205" height="534" alt="image" src="https://github.com/user-attachments/assets/c72e4eba-4005-41c4-8e3c-5991eb4e78fc" />

# LangGraph  ü¶úüîó

## ‚ñ∂Ô∏è Getting Started

### üõ†Ô∏è Prerequisites
- **This is not a beginner course** - Basic software engineering concepts needed
- Familiarity with: git, Python, environment variables, classes, testing and debugging
- Python 3.10+
- Any Python package manager (uv, poetry, pipenv) - but NOT conda!
- Access to an LLM (can be open source via Ollama, or cloud providers like OpenAI, Anthropic, Gemini)
- No Machine Learning experience needed

### ‚öôÔ∏è Setup Instructions

```bash
    # Setup Virtual Environment
    pipenv shell
   
    # Install dependencies
   pipenv install langchain && \
   pipenv install langchain-ollama  && \
   pipenv install langchain-community  && \
   pipenv install langchain-core  && \
   pipenv install langchainhub  && \
   pipenv install langchain_tavily && \
   pipenv install langsmith

    # JIRA/Confluence integration
   pipenv install jira python-dotenv openpyxl python-docx PyPDF2 python-magic atlassian-python-api html2text libmagic
   pipenv install torch transformers pytesseract pillow speechrecognition moviepy
   
   # image transformation
   brew install tesseract
   tesseract --version
   
   # code formatter tool
   pipenv install black
   
   # environment variable loader
   pipenv install python-dotenv
   
   # Testing and debugging
    pipenv install pytest
```

## üôè Reference

- https://python.langchain.com/docs/introduction/
- https://python.langchain.com/docs/tutorials/llm_chain/
- https://langchain-ai.github.io/langgraph/tutorials/introduction/
- https://python.langchain.com/docs/integrations/chat/



<img width="1645" height="751" alt="image" src="https://github.com/user-attachments/assets/e8b56e63-c736-40e4-ae5b-72f18e664d46" />


# Prompt Engineering
## Best practices for prompt engineering:
	‚Ä¢ Be clear and specific: Clearly state the task, context, and desired output.
	‚Ä¢ Use examples: Provide input-output pairs to guide the model.
	‚Ä¢ Set constraints: Specify format, length, or style requirements.
	‚Ä¢ Break down complex tasks: Use step-by-step instructions or chain-of-thought prompts.
	‚Ä¢ Iterate and refine: Test, analyze outputs, and adjust prompts for better results.
	‚Ä¢ Avoid ambiguity: Use precise language and avoid vague terms.
	‚Ä¢ Use system messages: Set behavior or tone if supported (e.g., ‚ÄúYou are a helpful assistant.‚Äù).
	‚Ä¢ Leverage role prompting: Assign roles to guide responses (e.g., ‚ÄúAct as a senior Java developer.‚Äù).
	‚Ä¢ Test edge cases: Ensure prompts handle unusual or unexpected inputs.
	‚Ä¢ Document prompts: Keep track of prompt versions and their effectiveness.


## Prompt engineering format
	‚Ä¢ Role: Specify which role or capabilities the AI should assume.
	‚Ä¢ Tasks/Instruction: Clearly state what you want the model to do.
	‚Ä¢ Context: Provide background information or relevant details.
	‚Ä¢ Input Data: Supply any data or examples needed for the task.
	‚Ä¢ Goal: Define what is the outcome of the task should achieve.
	‚Ä¢ Constraints: Specify requirements (e.g., output format, length, style).
	‚Ä¢ Output Format: Determine how the result should be presented.


## Core prompting techniques for LLMs
	‚Ä¢ Zero-shot prompting: Ask the model to perform a task without providing any examples.
	‚Ä¢ Few-shot prompting: Provide a few input-output examples to guide the model‚Äôs behavio
	‚Ä¢ Chain-of-thought (CoT) prompting: Encourage the model to reason step-by-step by explicitly asking for intermediate steps.


